{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "In this assignment you will learn to implement logistic regression and apply it on a toy dataset. Logistic regression is a popular machine learning technique used for binary classification. In a binary classification problem, each input belongs to one of two classes, say 0 or 1, and the goal is to predict the correct class of each input. \n",
    "\n",
    "While logistic regression is a standard machine learning algorithm used for binary classification, it can also be thought of as a simple neural network. Hence, we chose logistic regression for this exercise since it will help you understand some of the basic principles of machine learning and neural networks.\n",
    "\n",
    "#### Instructions\n",
    "-  Do not use any additional libraries other than what is already specified below\n",
    "-  Do not use loops in your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "For this assignment we will import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sigmoid\n",
    "Let us start this assignment by implementing the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # You need to apply the sigmoid function on each element of z and return the values\n",
    "\n",
    "    # Implement the sigmoid function below\n",
    "    \n",
    "    s = 1.0/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test if the sigmoid function is implemented correctly**\n",
    "\n",
    "Check that for input values 10,  -5,   0,   5,  and 10 you get the expected output as given in the table below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [-10  -5   0   5  10]\n",
      "Expected output:  [  4.53978687e-05   6.69285092e-03   5.00000000e-01   9.93307149e-01\n",
      "   9.99954602e-01]\n"
     ]
    }
   ],
   "source": [
    "z = np.arange(-10,11,5)\n",
    "sig = sigmoid(z)\n",
    "print \"Input: \",z\n",
    "print \"Expected output: \",sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th>Input</th>      <th>Expected output</th>    </tr>  </thead>  <tbody>    <tr>      <td>-10.0</td>      <td>0.000045</td>    </tr>    <tr>      <td>-5.0</td>      <td>0.006693</td>    </tr>    <tr>      <td>0.0</td>      <td>0.500000</td>    </tr>    <tr>      <td>5.0</td>      <td>0.993307</td>    </tr>    <tr>      <td>10.0</td>      <td>0.999955</td>    </tr>  </tbody></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us also visualize the sigmoid function**\n",
    "\n",
    "<img src=\"sigmoid.jpg\" align=\"left\" alt=\"Expected plot of sigmoid function\" title=\"Expected plot of sigmoid function\" />\n",
    "<p style=\"clear:left\">\n",
    "The sigmoid function you plot should look like the above figure.\n",
    "Run the cell below to visualize the sigmoid function you have implemented.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGQghbuISAZSuIuIZCCFu4hIBvr/11Tz8rxmsRUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19fda21790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.arange(-10., 11., 1)\n",
    "sig = sigmoid(z)\n",
    "plt.plot(z,sig)\n",
    "plt.plot(z,sig,'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Representation and Parameters\n",
    "Assuming there are $m$ training samples, the pair $(x^{(i)}, \\hat{y}^{(i)})$ denote the $i^{th}$ training sample. Each training sample consists of $n$ features denoted by $x^{(i)}$ and the correponding class label $\\hat{y}^{(i)}$. For each training sample, the features $x^{(i)}$ can be thought of as an $n\\times1$ column vector. When we use $n$ features for representing an input, the number of parameters in logistic regression becomes $n+1$ where the first $n$ parameters are the weights $w$ and the last parameter is the bias term $b$. The optimal values for these parameters are determined during the training phase which consists of forward propagation and back propagation such that the parameters maximize the accuracy of predicting the values of $\\hat{y}^{(i)}$.\n",
    "\n",
    "For vectorized operations, we arrange the column vectors corresponding to all the features of the training samples in the form of a $n \\times m$ matrix $X$, the ground truth class labels as a $1 \\times m$ row vector $\\hat{y}$, and the weight parameters $w$ as a $n \\times 1$ column vector. Here is an example of three training samples each containing two features with class labels 0, 1, 0 respectively.\n",
    "$\n",
    "\\begin{align*}\n",
    "    x^{(1)} &= \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           2 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "    x^{(2)} &= \\begin{bmatrix}\n",
    "           3 \\\\\n",
    "           4 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "     x^{(3)} &= \\begin{bmatrix}\n",
    "       5 \\\\\n",
    "       6 \\\\\n",
    "     \\end{bmatrix}.\n",
    "     &\n",
    "     \\textrm{Then,}\\;\n",
    "     X &= \\begin{bmatrix}\n",
    "           1\\;3\\;5 \\\\\n",
    "           2\\;4\\;6\\\\\n",
    "         \\end{bmatrix},\n",
    "     &\n",
    "     \\hat y &= \\begin{bmatrix}\n",
    "           0\\;1\\;0\n",
    "         \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "And the parameters to be estimated are the weights\n",
    "$\n",
    "\\begin{align*}\n",
    "     w &= \\begin{bmatrix}\n",
    "           w_1\\\\\n",
    "           w_2\\\\\n",
    "         \\end{bmatrix}, \n",
    "\\end{align*}\n",
    "$\n",
    "and the bias $b$.\n",
    "\n",
    "\n",
    " \n",
    "Use the cell below to create the parameters $w$ and $b$ and initialize them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_params(n):\n",
    "    # Create the parameter w with the correct dimension and initialise it with zeros using np.zeros\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w,b = create_params(2)\n",
    "\n",
    "print w\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody><tr style=\"text-align: left;\"><td>[ [ 0. ]<br/> [ 0. ] ]<br/> 0.0  </td> </tr></tbody>  </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the rest of the code let us assign some values for our data and parameters in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, b, X, yhat = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Forward and Backward Propagation\n",
    "\n",
    "To train our simple neural network i.e., to estimate the parameters, we need to do forward and backward propagation. \n",
    "\n",
    "During forward propagation, you need to compute:\n",
    "1. The neuron activation for the $i^{th}$ sample: $a^{(i)} = sigmoid( \\sum_{j=1}^{n} w_jx^{(i)}_j + b) $.\n",
    "2. The loss function which indicates how good the current value of the parameters are:  \n",
    "$L = -\\frac{1}{m}\\sum_{i=1}^{m}\\hat y^{(i)}\\log(a^{(i)})+(1-\\hat y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "During backward propagation, you need to compute:\n",
    "1. The gradient of the loss function with respect to each of the weight parameter w$_j$: $ dw_j = \\frac{\\partial L}{\\partial w_j} = \\frac{1}{m}\\sum_{i=1}^{m}x_j^{(i)} (a^{(i)}-\\hat y^{(i)})$\n",
    "2. The gradient of the loss function with respect to the bias b: $ db = \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-\\hat y^{(i)})$\n",
    "\n",
    "You need to implement the forward and backward propagation in the cells below. You should not use any loops but instead use vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X,yhat,w,b):\n",
    "    a = sigmoid(np.dot(w.T,X)+b\n",
    "    loss = -np.mean(yhat*np.log(a) + (1.0-yhat)*np.log(1.0-a))\n",
    "    return a,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test if the forward propagation is implemented correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99987661  0.99999386]]\n",
      "-6.00006477319\n"
     ]
    }
   ],
   "source": [
    "activation,loss = forward_propagate(X,yhat,w,b)\n",
    "print activation\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody><tr style=\"text-align: left;\"><td>[[ 0.99987661  0.99999386]]\n",
    "<br/> -6.00006477319<br/> </td> </tr></tbody>  </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagate(X,yhat,a):\n",
    "    m = X.shape[1]\n",
    "    dw = 1.0/m*np.dot(X,(a-yhat).T)\n",
    "    db = 1.0/m*np.sum(a-yhat)\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test if the backward propagation is implemented correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99993216]\n",
      " [ 1.99980262]]\n",
      "0.499935230625\n"
     ]
    }
   ],
   "source": [
    "dw, db = backward_propagate(X,yhat,activation)\n",
    "print dw\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\"> <th>Expected output:</th></tr></thead> <tbody><tr style=\"text-align: left;\"><td>[[ 0.99993216] <br/> [ 1.99980262]] <br/> 0.499935230625 </td> </tr></tbody>  </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X,yhat):\n",
    "    n = X.shape[0]\n",
    "    w,b = create_params(n)\n",
    "    alpha = 0.005\n",
    "    for i in range(2000):\n",
    "        activation, loss = forward_propagate(X,yhat,w,b)\n",
    "        dw,db = backward_propagate(X,yhat,activation)\n",
    "        w -= alpha*dw\n",
    "        b -= alpha*db\n",
    "        if i % 100 == 0:\n",
    "            print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (1, 2)\n",
      "-0.6931471805599453\n",
      "-0.6691450350595828\n",
      "-0.6577972607668807\n",
      "-0.6469398689225863\n",
      "-0.6363347464195545\n",
      "-0.6259716079871211\n",
      "-0.6158447844841767\n",
      "-0.6059487644141899\n",
      "-0.5962781111842828\n",
      "-0.5868274661576235\n",
      "-0.5775915525537973\n",
      "-0.5685651788233299\n",
      "-0.5597432415440944\n",
      "-0.5511207278757766\n",
      "-0.542692717602205\n",
      "-0.5344543847897911\n",
      "-0.5264009990894083\n",
      "-0.5185279267081349\n",
      "-0.5108306310763145\n",
      "-0.5033046732343502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print w.shape, yhat.shape\n",
    "fit(X,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
