{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression\n",
    "\n",
    "In this assignment you will learn to implement logistic regression and apply it on a toy dataset. Logistic regression is a popular machine learning technique used for binary classification. In a binary classification problem, each input belongs to one of two classes, say 0 or 1, and the goal is to predict the correct class of each input. \n",
    "\n",
    "While logistic regression is a standard machine learning algorithm used for binary classification, it can also be thought of as a simple neural network. Hence, we chose logistic regression for this exercise since it will help you understand some of the basic principles of machine learning and neural networks.\n",
    "\n",
    "#### Instructions\n",
    "-  Do not use any additional libraries other than what is already specified below\n",
    "-  Do not use loops in your code\n",
    "-  Do not edit \"Expected output\" markdown cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "For this assignment we will import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sigmoid\n",
    "Let us start this assignment by implementing the sigmoid function: sigmoid(s) = $\\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # Implement the sigmoid function\n",
    "    \n",
    "    # Arguments:\n",
    "    #    z: A set of values whose sigmoid needs to be computed and stored in the variable s\n",
    "    #\n",
    "    # Return value: \n",
    "    #    s: sigmoid of values in z \n",
    "    \n",
    "    # Implement the sigmoid function: s = 1/(1+e^(-z))\n",
    "    \n",
    "    s = \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Let us test if the sigmoid function is implemented correctly\n",
    "\n",
    "Check that for input values 10,  -5,   0,   5,  and 10 you get the expected output as given in the table below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10,11,5)\n",
    "sig = sigmoid(z)\n",
    "print \"Input: \",z\n",
    "print \"Expected output: \",sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th>Input</th>      <th>Expected output</th>    </tr>  </thead>  <tbody>    <tr>      <td>-10.0</td>      <td>0.000045</td>    </tr>    <tr>      <td>-5.0</td>      <td>0.006693</td>    </tr>    <tr>      <td>0.0</td>      <td>0.500000</td>    </tr>    <tr>      <td>5.0</td>      <td>0.993307</td>    </tr>    <tr>      <td>10.0</td>      <td>0.999955</td>    </tr>  </tbody></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us also visualize the sigmoid function**\n",
    "\n",
    "<img src=\"sigmoid.jpg\" align=\"left\" alt=\"Expected plot of sigmoid function\" title=\"Expected plot of sigmoid function\" />\n",
    "<p style=\"clear:left\">\n",
    "The sigmoid function you plot should look like the above figure.\n",
    "Run the cell below to visualize the sigmoid function you have implemented.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-10., 11., 1)\n",
    "sig = sigmoid(z)\n",
    "plt.plot(z,sig)\n",
    "plt.plot(z,sig,'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Representation and Parameters\n",
    "Assuming there are $m$ training samples, the pair $(x^{(i)}, \\hat{y}^{(i)})$ denote the $i^{th}$ training sample. Each training sample consists of $n$ features denoted by $x^{(i)}$ and the correponding class label $\\hat{y}^{(i)}$. For each training sample, the features $x^{(i)}$ can be thought of as an $n\\times1$ column vector. When we use $n$ features for representing an input, the number of parameters in logistic regression becomes $n+1$ where the first $n$ parameters are the weights $w$ and the last parameter is the bias term $b$. The optimal values for these parameters are determined during the training phase which consists of forward propagation and back propagation.\n",
    "\n",
    "For vectorized operations, we arrange the column vectors corresponding to all the features of the training samples in the form of a $n \\times m$ matrix $X$, the ground truth class labels as a $1 \\times m$ row vector $\\hat{y}$, and the weight parameters $w$ as a $n \\times 1$ column vector. Here is an example of three training samples each containing two features with class labels 0, 1, 0 respectively.\n",
    "$\n",
    "\\begin{align*}\n",
    "    x^{(1)} &= \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           2 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "    x^{(2)} &= \\begin{bmatrix}\n",
    "           3 \\\\\n",
    "           4 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "     x^{(3)} &= \\begin{bmatrix}\n",
    "       5 \\\\\n",
    "       6 \\\\\n",
    "     \\end{bmatrix}.\n",
    "     &\n",
    "     \\textrm{Then,}\\;\n",
    "     X &= \\begin{bmatrix}\n",
    "           1\\;3\\;5 \\\\\n",
    "           2\\;4\\;6\\\\\n",
    "         \\end{bmatrix},\n",
    "     &\n",
    "     \\hat y &= \\begin{bmatrix}\n",
    "           0\\;1\\;0\n",
    "         \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "The parameters to be estimated are the weights\n",
    "$\n",
    "\\begin{align*}\n",
    "     w &= \\begin{bmatrix}\n",
    "           w_1\\\\\n",
    "           w_2\\\\\n",
    "         \\end{bmatrix}, \n",
    "\\end{align*}\n",
    "$\n",
    "and the bias $b$.\n",
    "\n",
    "\n",
    " \n",
    "Use the cell below to create the parameters $w$ and $b$ and initialize them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(n):\n",
    "    # Create the parameters w and b and initialise them with zeros\n",
    "    \n",
    "    # Arguments:\n",
    "    #    n: number of weights\n",
    "    #\n",
    "    # Return values:\n",
    "    #    w: the weights created  \n",
    "    #    b: bias term\n",
    "    \n",
    "    # Create a numpy array w of shape (n,1) which are initialized to zeros \n",
    "    w = \n",
    "    # Initialise bias term b to zero\n",
    "    b = \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Let us verify if create_params is creating the parameters correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = create_params(2)\n",
    "\n",
    "print w\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: right;\"><td>  [ [0. ] <br/>&nbsp; [0. ] ] <br/> 0.0   </td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Forward Propagation\n",
    "  \n",
    "To train our simple neural network i.e., to estimate the parameters, we need to do forward and backward propagation. During forward propagation, you determine using a loss function how good the current value of the parameters are. The loss function for logistic regression (our simple neural network) using $m$ training samples is:\n",
    "\n",
    "$L = -\\frac{1}{m}\\sum_{i=1}^{m}\\hat y^{(i)}\\log(a^{(i)})+(1-\\hat y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "where $a^{(i)} = sigmoid( \\sum_{j=1}^{n} w_jx^{(i)}_j + b) $ is the activation for the i-th training sample.\n",
    "\n",
    "You have to implement the forward propagation and return the value of the activation and loss using vectorized implementations (no loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X,yhat,w,b):\n",
    "    # Implement forward propagation\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X    : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    w    : the n weight parameters\n",
    "    #    b    : the bias term b\n",
    "    #\n",
    "    # Return values:\n",
    "    #    a    : the activation of all the training samples calculated according to the formula above\n",
    "    #    loss : the loss calculated according to the formula above\n",
    "    \n",
    "    \n",
    "    # Calculate the activation using sigmoid function based on the formula above. You need to store \n",
    "    # the activation for all the training elements in an appropriate matrix.\n",
    "    a = \n",
    "    \n",
    "    # Calculate the loss using the equation for L above\n",
    "    loss = \n",
    "    \n",
    "    \n",
    "    return a,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "To test forward and backward propagation, let us use the values below for $w, b, X,$ and $\\hat y$. Check that the activation and loss returned by the forward_propagate function matches the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]]) \n",
    "yhat = np.array([[1, 0, 1]])\n",
    "a,loss = forward_propagate(X,yhat,w,b)\n",
    "print a\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: right;\"><td>  [[0.98201379 0.99752738 0.99966465]] <br/>2.006987006476161</td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Backward Propagation\n",
    "\n",
    "Now let us implement the backward propagation where you compute the derivatives of the loss function with respect to the parameters $w$ and $b$. The derivatives are given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_j} &= \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}_j \\cdot (a^{(i)}-\\hat y^{(i)})\\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\n",
    "\\end{align}\n",
    "You should write a vectorized implementation without using any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(X,yhat,a):\n",
    "    # Implement backward propagation using the equations above without using any loops\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X    : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    a    : the activation of the training samples\n",
    "    #\n",
    "    # Return values:\n",
    "    #    dw   : derivative of loss with respect to w\n",
    "    #    db   : derivative of loss with respect to b\n",
    "    \n",
    "    # Determine the number of trainining samples m from the shape of X\n",
    "    m = \n",
    "    # Calculate dw = dL/dw using the equation above without using any loops\n",
    "    dw = \n",
    "    # Calculate db = dL/db using the equation above without using any loops\n",
    "    db = \n",
    "    \n",
    "    #return derivative of loss with respect to w and b, i.e., dw and db\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]]) \n",
    "yhat = np.array([[1, 0, 1]])\n",
    "dw,db = backward_propagate(X,yhat,a)\n",
    "print dw\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: left;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: left;\"><td>  [[0.99097306]<br/>[1.317375  ]]<br/>0.3264019389169358</td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Finding the optimal value for the parameters\n",
    "\n",
    "Using the forward and backward propagation that was implemented above, let us now write the function that will do gradient descent and determine the optimal value of the parameters that minimizes the loss. Let us call this function fit. It takes as arguments the input $X$, the groundtruth labels $\\hat y$, the number of iterations to run, and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,yhat,numiter,lr,display_loss = False):\n",
    "    # Implement the fit fuction that determines optimal values for the parameters w and b using gradient descent.\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X            : n features of m training samples represented as an nxm matrix\n",
    "    #    yhat         : the groundtruth values for the m training samples represented as an 1xm matrix\n",
    "    #    numiter      : number of iterations for running the gradient descent \n",
    "    #    lr           : learning rate for the gradient descent\n",
    "    #    display_loss : whether to display loss\n",
    "    #\n",
    "    # Return values:\n",
    "    #    w            : weights determined by the gradient descent\n",
    "    #    b            : bias determined by the gradient descent\n",
    "    \n",
    "    # Determine the number of features from the shape of X\n",
    "    n = \n",
    "    \n",
    "    # Create the weights and bias\n",
    "    w,b = create_params(n)\n",
    "    \n",
    "    # Iterate for numiter steps\n",
    "    for i in range(numiter):\n",
    "        \n",
    "        # Calculate activation and loss using forward propagation\n",
    "        a,loss = \n",
    "        \n",
    "        # Calculate dw and db using backward propagation\n",
    "        dw,db = \n",
    "        \n",
    "        # Calculate the updated parameters by moving a small step in the direction of the gradient\n",
    "        w = w - \n",
    "        b = b - \n",
    "        \n",
    "        if display_loss and i % 100 == 0:\n",
    "            print loss\n",
    "            \n",
    "    # Return the parameters w and b\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = fit(X,yhat,500,0.005,True)\n",
    "print w\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: left;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: left;\"><td>  0.6931471805599453<br/>\n",
    "1.0019505793318735<br/>\n",
    "0.7506026263399358<br/>\n",
    "0.8973193861672867<br/>\n",
    "0.9111127740792093<br/>\n",
    "    [[0.02845113]<br/>\n",
    " [0.10866157]] <br/>\n",
    "    0.08021044325121013\n",
    "    </td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inference using the learned parameters\n",
    "Finally, let us write a function that will make predictions for a new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w,b):\n",
    "    # Implement the function to predict the output for a new input using the learned parameters.\n",
    "    \n",
    "    # Arguments:\n",
    "    #    X    : n features of m training samples represented as an nxm matrix\n",
    "    #    w    : weights determined by the training\n",
    "    #    b    : bias determined by the training\n",
    "    #\n",
    "    # Return values:\n",
    "    #    pred : prediction for each test sample\n",
    "    \n",
    "    \n",
    "    # Determine the activation for each input test sample\n",
    "    act = \n",
    "    \n",
    "    # Since we are using a binary classifer, the output has to be either a 0 or a 1, so you need\n",
    "    # to convert the activation to a 0 or a 1.\n",
    "    pred = \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]])\n",
    "pred = predict(X,w,b)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: left;\">      <th>Expected output</th>    </tr>  </thead>\n",
    "    <tbody>\n",
    "        <tr style=\"text-align: left;\">\n",
    "            <td>\n",
    "                [[1. 1. 1.]]<br/>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting it all together\n",
    "\n",
    "Now let us put it all together to classify a simple dataset. Let us first create a simple dataset, split it into training and testing sets and visualise it. For this, we will use the code from http://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,yhat_train,X_test,yhat_test = utils.create_simple_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the training set, let us estimate the parameters and use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w,b = fit(X_train,yhat_train,5000,0.05)\n",
    "\n",
    "res = predict(X_test,w,b)\n",
    "\n",
    "print \"Accuracy of prediction: \" + str(np.sum(res == yhat_test)/float(res.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.vis_classifier(X_test,yhat_test,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
