{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression\n",
    "\n",
    "In this assignment you will learn to implement logistic regression and apply it on a toy dataset. Logistic regression is a popular machine learning technique used for binary classification. In a binary classification problem, each input belongs to one of two classes, say 0 or 1, and the goal is to predict the correct class of each input. \n",
    "\n",
    "While logistic regression is a standard machine learning algorithm used for binary classification, it can also be thought of as a simple neural network. Hence, we chose logistic regression for this exercise since it will help you understand some of the basic principles of machine learning and neural networks.\n",
    "\n",
    "#### Instructions\n",
    "-  Do not use any additional libraries other than what is already specified below\n",
    "-  Do not use loops in your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "For this assignment we will import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sigmoid\n",
    "Let us start this assignment by implementing the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # You need to apply the sigmoid function on each element of z and return the values\n",
    "\n",
    "    # Implement the sigmoid function below\n",
    "    \n",
    "    s = 1.0/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Let us test if the sigmoid function is implemented correctly\n",
    "\n",
    "Check that for input values 10,  -5,   0,   5,  and 10 you get the expected output as given in the table below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [-10  -5   0   5  10]\n",
      "Expected output:  [4.53978687e-05 6.69285092e-03 5.00000000e-01 9.93307149e-01\n",
      " 9.99954602e-01]\n"
     ]
    }
   ],
   "source": [
    "z = np.arange(-10,11,5)\n",
    "sig = sigmoid(z)\n",
    "print \"Input: \",z\n",
    "print \"Expected output: \",sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th>Input</th>      <th>Expected output</th>    </tr>  </thead>  <tbody>    <tr>      <td>-10.0</td>      <td>0.000045</td>    </tr>    <tr>      <td>-5.0</td>      <td>0.006693</td>    </tr>    <tr>      <td>0.0</td>      <td>0.500000</td>    </tr>    <tr>      <td>5.0</td>      <td>0.993307</td>    </tr>    <tr>      <td>10.0</td>      <td>0.999955</td>    </tr>  </tbody></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us also visualize the sigmoid function**\n",
    "\n",
    "<img src=\"sigmoid.jpg\" align=\"left\" alt=\"Expected plot of sigmoid function\" title=\"Expected plot of sigmoid function\" />\n",
    "<p style=\"clear:left\">\n",
    "The sigmoid function you plot should look like the above figure.\n",
    "Run the cell below to visualize the sigmoid function you have implemented.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb991f43790>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.arange(-10., 11., 1)\n",
    "sig = sigmoid(z)\n",
    "plt.plot(z,sig)\n",
    "plt.plot(z,sig,'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Representation and Parameters\n",
    "Assuming there are $m$ training samples, the pair $(x^{(i)}, \\hat{y}^{(i)})$ denote the $i^{th}$ training sample. Each training sample consists of $n$ features denoted by $x^{(i)}$ and the correponding class label $\\hat{y}^{(i)}$. For each training sample, the features $x^{(i)}$ can be thought of as an $n\\times1$ column vector. When we use $n$ features for representing an input, the number of parameters in logistic regression becomes $n+1$ where the first $n$ parameters are the weights $w$ and the last parameter is the bias term $b$. The optimal values for these parameters are determined during the training phase which consists of forward propagation and back propagation so that the parameters maximize the accuracy of predicting the values of $\\hat{y}^{(i)}$.\n",
    "\n",
    "For vectorized operations, we arrange the column vectors corresponding to all the features of the training samples in the form of a $n \\times m$ matrix $X$, the ground truth class labels as a $1 \\times m$ row vector $\\hat{y}$, and the weight parameters $w$ as a $n \\times 1$ column vector. Here is an example of three training samples each containing two features with class labels 0, 1, 0 respectively.\n",
    "$\n",
    "\\begin{align*}\n",
    "    x^{(1)} &= \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           2 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "    x^{(2)} &= \\begin{bmatrix}\n",
    "           3 \\\\\n",
    "           4 \\\\\n",
    "         \\end{bmatrix};\n",
    "         &\n",
    "     x^{(3)} &= \\begin{bmatrix}\n",
    "       5 \\\\\n",
    "       6 \\\\\n",
    "     \\end{bmatrix}.\n",
    "     &\n",
    "     \\textrm{Then,}\\;\n",
    "     X &= \\begin{bmatrix}\n",
    "           1\\;3\\;5 \\\\\n",
    "           2\\;4\\;6\\\\\n",
    "         \\end{bmatrix},\n",
    "     &\n",
    "     \\hat y &= \\begin{bmatrix}\n",
    "           0\\;1\\;0\n",
    "         \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "And the parameters to be estimated are the weights\n",
    "$\n",
    "\\begin{align*}\n",
    "     w &= \\begin{bmatrix}\n",
    "           w_1\\\\\n",
    "           w_2\\\\\n",
    "         \\end{bmatrix}, \n",
    "\\end{align*}\n",
    "$\n",
    "and the bias $b$.\n",
    "\n",
    "\n",
    " \n",
    "Use the cell below to create the parameters $w$ and $b$ and initialize them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(n):\n",
    "    # Create the parameters w and b and initialise them with zeros\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Let us verify if create_params is creating the parameters correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w,b = create_params(2)\n",
    "\n",
    "print w\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: right;\"><td>  [ [0. ] <br/>&nbsp; [0. ] ] <br/> 0.0   </td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Forward Propagation\n",
    "  \n",
    "To train our simple neural network i.e., to estimate the parameters, we need to do forward and backward propagation. During forward propagation, you determine using a loss function how good the current value of the parameters are. The loss function for logistic regression (our simple neural network) using $m$ training samples is:\n",
    "\n",
    "$L = -\\frac{1}{m}\\sum_{i=1}^{m}\\hat y^{(i)}\\log(a^{(i)})+(1-\\hat y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "where $a^{(i)} = sigmoid( \\sum_{j=1}^{n} w_jx^{(i)}_j + b) $ is the activation for the i-th training sample.\n",
    "\n",
    "You have to implement the forward propagation and return the value of the activation and loss using vectorized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X,yhat,w,b):\n",
    "    # Implement forward propagation by \n",
    "    #     1. calculating the activation using sigmoid function\n",
    "    #     2. calculating the loss using the equation for L above\n",
    "    \n",
    "    a = sigmoid(np.dot(w.T,X)+b)\n",
    "    loss = -np.mean(yhat*np.log(a) + (1.0-yhat)*np.log(1.0-a))\n",
    "    \n",
    "    # return the values of activation and loss\n",
    "    return a,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "To test forward and backward propagation, let us use the values below for $w, b, X,$ and $\\hat y$. Check that the activation and loss returned by the forward_propagate function matches the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98201379 0.99752738 0.99966465]]\n",
      "2.006987006476161\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]]) \n",
    "yhat = np.array([[1, 0, 1]])\n",
    "a,loss = forward_propagate(X,yhat,w,b)\n",
    "print a\n",
    "print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: right;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: right;\"><td>  [[0.98201379 0.99752738 0.99966465]] <br/>2.006987006476161</td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Backward Propagation\n",
    "\n",
    "Now let us implement the backward propagation where you compute the derivatives of the loss function with respect to the parameters $w$ and $b$. The derivatives are given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_j} &= \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}_j \\cdot (a^{(i)}-\\hat y^{(i)})\\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\n",
    "\\end{align}\n",
    "You should write a vectorized implementation without using any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(X,yhat,a):\n",
    "    # Implement backward propagation using the equations above without using any loops\n",
    "    m = X.shape[1]\n",
    "    dw = 1.0/m*np.dot(X,(a-yhat).T)\n",
    "    db = 1.0/m*np.sum(a-yhat)\n",
    "    \n",
    "    #return derivative of loss with respect to w and b, i.e., dw and db\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99097306]\n",
      " [1.317375  ]]\n",
      "0.3264019389169358\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]]) \n",
    "yhat = np.array([[1, 0, 1]])\n",
    "dw,db = backward_propagate(X,yhat,a)\n",
    "print dw\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: left;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: left;\"><td>  [[0.99097306]<br/>[1.317375  ]]<br/>0.3264019389169358</td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Finding the optimal value for the parameters\n",
    "\n",
    "Using the forward and backward propagation that was implemented above, let us now write the function that will do gradient descent and determine the optimal value of the parameters that minimizes the loss. Let us call this function fit. It takes as arguments the input $X$, the groundtruth labels $\\hat y$, the number of iterations to run, and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,yhat,numiter,lr):\n",
    "    # Implement the fit fuction that determines optimal values for the parameters w and b using gradient descent.\n",
    "    \n",
    "    # Create the parameters\n",
    "    n = X.shape[0]\n",
    "    w,b = create_params(n)\n",
    "    \n",
    "    # Iterate for numiter\n",
    "    for i in range(numiter):\n",
    "        # Calculate activation and loss using forward propagation\n",
    "        a,loss = forward_propagate(X,yhat,w,b)\n",
    "        \n",
    "        # Calculate dw and db using backward propagation\n",
    "        dw,db = backward_propagate(X,yhat,a)\n",
    "        \n",
    "        # Update the parameters\n",
    "        w -= lr*dw\n",
    "        b -= lr*db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "1.0019505793318735\n",
      "0.7506026263399358\n",
      "0.8973193861672867\n",
      "0.9111127740792093\n"
     ]
    }
   ],
   "source": [
    "fit(X,yhat,500,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\"> <thead>    <tr style=\"text-align: left;\">      <th>Expected output</th>    </tr>  </thead><tbody> <tr style=\"text-align: left;\"><td>  0.6931471805599453<br/>\n",
    "1.0019505793318735<br/>\n",
    "0.7506026263399358<br/>\n",
    "0.8973193861672867<br/>\n",
    "0.9111127740792093<br/></td></tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inference using the learned parameters\n",
    "Finally, let us write a function that will make predictions for a new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w,b):\n",
    "    # Implement the function to predict the output for a new input using the learned parameters.\n",
    "    # Since we are using a binary classifer, the output has to be either a 0 or a 1, so you need\n",
    "    # to convert the output to a 0 or a 1.\n",
    "    act = sigmoid(np.dot(w.T,X)+b)\n",
    "    pred = np.round(act)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing \n",
    "Check that the results match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0], [1]])\n",
    "b = 2\n",
    "X = np.array([[1, 3, 5],[2, 4, 6]])\n",
    "pred = predict(X,w,b)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
